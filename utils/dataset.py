"""
–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ LiDAR –¥–∞–Ω–Ω—ã—Ö –∏–∑ LAS —Ñ–∞–π–ª–æ–≤
–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å laspy 2.x –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
"""

import numpy as np
import torch
from torch.utils.data import Dataset
import laspy
from pathlib import Path
from tqdm import tqdm
import pickle
import os


class LASDataset(Dataset):
    """
    –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è LiDAR –¥–∞–Ω–Ω—ã—Ö –∏–∑ LAS —Ñ–∞–π–ª–æ–≤
    –†–∞–∑–±–∏–≤–∞–µ—Ç –æ–±–ª–∞–∫–æ —Ç–æ—á–µ–∫ –Ω–∞ –±–ª–æ–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    """
    
    def __init__(
        self,
        las_file,
        num_points=4096,
        block_size=50.0,
        stride=None,
        use_features=True,
        normalize=True,
        augment=False,
        cache_dir='cache',
        dataset_config=None 
    ):
        """
        Args:
            las_file: –ø—É—Ç—å –∫ LAS —Ñ–∞–π–ª—É
            num_points: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –≤ –±–ª–æ–∫–µ
            block_size: —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –≤ –º–µ—Ç—Ä–∞—Ö
            stride: —à–∞–≥ –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é block_size/2)
            use_features: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –¥–æ–ø. –ø—Ä–∏–∑–Ω–∞–∫–∏ (intensity, returns)
            normalize: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏
            augment: –ø—Ä–∏–º–µ–Ω—è—Ç—å –ª–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            cache_dir: –ø–∞–ø–∫–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
            dataset_config: DatasetConfig –æ–±—ä–µ–∫—Ç –∏–ª–∏ –ø—É—Ç—å –∫ YAML —Ñ–∞–π–ª—É
        """
        self.las_file = las_file
        self.num_points = num_points
        self.block_size = block_size
        self.stride = stride if stride is not None else block_size / 2
        self.use_features = use_features
        self.normalize = normalize
        self.augment = augment
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        if dataset_config is None:
            # –ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            try:
                from utils.dataset_config import auto_detect_config
                dataset_config = auto_detect_config(las_file)
            except:
                pass
            
            if dataset_config is None:
                # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º NEON –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
                print("‚ö†Ô∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NEON –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
                try:
                    from utils.dataset_config import DatasetConfig
                    dataset_config = DatasetConfig('configs/datasets/neon_sample.yaml')
                except:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–Ω—Ñ–∏–≥–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–∞–ø–ø–∏–Ω–≥
                    print("‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤")
                    dataset_config = None
        
        elif isinstance(dataset_config, (str, Path)):
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞
            from utils.dataset_config import DatasetConfig
            dataset_config = DatasetConfig(dataset_config)
        
        self.dataset_config = dataset_config
        
        # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤
        if dataset_config is not None:
            self.class_mapping = dataset_config.class_mapping
            self.num_classes = dataset_config.num_classes
        else:
            # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –º–∞–ø–ø–∏–Ω–≥
            self.class_mapping = {1: 0, 2: 1, 5: 2, 6: 3}
            self.num_classes = 4
        
        print(f"\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ LAS —Ñ–∞–π–ª–∞: {las_file}")
        if dataset_config is not None:
            print(f"üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {dataset_config.name}")
        
        self._load_data()
        self._create_blocks()
        
        print(f"\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤:")
        print(f"   ‚Ä¢ –ë–ª–æ–∫–æ–≤: {len(self.blocks)}")
        print(f"   ‚Ä¢ –¢–æ—á–µ–∫ –≤ –±–ª–æ–∫–µ: {self.num_points}")
        print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞: {self.block_size}m")
        print(f"   ‚Ä¢ Stride: {self.stride}m")
        print(f"   ‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–∏: {'XYZ + intensity + returns' if use_features else '–¢–æ–ª—å–∫–æ XYZ'}")
    
    def _load_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ LAS —Ñ–∞–π–ª–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cache_file = self.cache_dir / f"{Path(self.las_file).stem}_preprocessed.pkl"
        
        if cache_file.exists():
            print(f"   üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –∫—ç—à–∞: {cache_file}")
            with open(cache_file, 'rb') as f:
                cached = pickle.load(f)
                self.points = cached['points']
                self.labels = cached['labels']
                self.features = cached['features']
                self.bounds = cached['bounds']
            return
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ LAS
        print(f"   üìñ –ß—Ç–µ–Ω–∏–µ LAS —Ñ–∞–π–ª–∞...")
        las = laspy.read(self.las_file)
        
        # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        xyz = np.vstack([
            np.array(las.x, dtype=np.float32),
            np.array(las.y, dtype=np.float32),
            np.array(las.z, dtype=np.float32)
        ]).T
        
        # –ú–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤
        labels = np.array(las.classification, dtype=np.int32)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = {}
        
        # Intensity
        try:
            if hasattr(las, 'intensity'):
                features['intensity'] = np.array(las.intensity, dtype=np.float32)
        except:
            print("   ‚ö†Ô∏è  Intensity –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # Return number
        try:
            if hasattr(las, 'return_number') or hasattr(las, 'return_num'):
                return_num = las.return_number if hasattr(las, 'return_number') else las.return_num
                features['return_number'] = np.array(return_num, dtype=np.float32)
        except:
            print("   ‚ö†Ô∏è  Return number –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # Number of returns
        try:
            if hasattr(las, 'number_of_returns') or hasattr(las, 'num_returns'):
                num_returns = las.number_of_returns if hasattr(las, 'number_of_returns') else las.num_returns
                features['number_of_returns'] = np.array(num_returns, dtype=np.float32)
        except:
            print("   ‚ö†Ô∏è  Number of returns –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Ç–æ—á–µ–∫: {len(xyz):,}")
        print(f"   ‚Ä¢ –ö–ª–∞—Å—Å—ã: {np.unique(labels)}")
        print(f"   ‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features)}")
        
        # üÜï –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if self.dataset_config is not None:
            labels_mapped = self.dataset_config.map_labels(labels)
        else:
            # –°—Ç–∞—Ä—ã–π –º–∞–ø–ø–∏–Ω–≥
            labels_mapped = np.full_like(labels, -1)
            for original, mapped in self.class_mapping.items():
                labels_mapped[labels == original] = mapped
        
        # –£–¥–∞–ª—è–µ–º —Ç–æ—á–∫–∏ —Å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏
        valid_mask = labels_mapped >= 0
        xyz = xyz[valid_mask]
        labels_mapped = labels_mapped[valid_mask]
        for key in features:
            features[key] = features[key][valid_mask]
        
        print(f"   ‚Ä¢ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(xyz):,} —Ç–æ—á–µ–∫")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
        unique_labels, counts = np.unique(labels_mapped, return_counts=True)
        print(f"\n   üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        total = len(labels_mapped)
        for label, count in zip(unique_labels, counts):
            percent = 100.0 * count / total
            if self.dataset_config:
                class_name = self.dataset_config.get_class_name(label)
                print(f"      –ö–ª–∞—Å—Å {label} ({class_name}): {count:,} —Ç–æ—á–µ–∫ ({percent:.2f}%)")
            else:
                print(f"      –ö–ª–∞—Å—Å {label}: {count:,} —Ç–æ—á–µ–∫ ({percent:.2f}%)")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –º–∏–Ω–∏–º—É–º—É)
        self.bounds = {
            'x_min': xyz[:, 0].min(),
            'y_min': xyz[:, 1].min(),
            'z_min': xyz[:, 2].min(),
            'x_max': xyz[:, 0].max(),
            'y_max': xyz[:, 1].max(),
            'z_max': xyz[:, 2].max(),
        }
        
        xyz[:, 0] -= self.bounds['x_min']
        xyz[:, 1] -= self.bounds['y_min']
        
        self.points = xyz
        self.labels = labels_mapped
        self.features = features
        
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        print(f"\n   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à: {cache_file}")
        with open(cache_file, 'wb') as f:
            pickle.dump({
                'points': self.points,
                'labels': self.labels,
                'features': self.features,
                'bounds': self.bounds
            }, f)
    
    def _create_blocks(self):
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫ –Ω–∞ –±–ª–æ–∫–∏"""
        cache_file = self.cache_dir / f"{Path(self.las_file).stem}_blocks_{self.block_size}_{self.stride}.pkl"
        
        if cache_file.exists():
            print(f"\n   üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –±–ª–æ–∫–æ–≤ –∏–∑ –∫—ç—à–∞: {cache_file}")
            with open(cache_file, 'rb') as f:
                self.blocks = pickle.load(f)
            return
        
        print(f"\n   üî® –°–æ–∑–¥–∞–Ω–∏–µ –±–ª–æ–∫–æ–≤...")
        
        x_min, y_min = 0, 0
        x_max = self.bounds['x_max'] - self.bounds['x_min']
        y_max = self.bounds['y_max'] - self.bounds['y_min']
        
        blocks = []
        
        x_start = x_min
        pbar = tqdm(desc="   –°–æ–∑–¥–∞–Ω–∏–µ –±–ª–æ–∫–æ–≤", unit="row")
        
        while x_start < x_max:
            y_start = y_min
            while y_start < y_max:
                # –ú–∞—Å–∫–∞ —Ç–æ—á–µ–∫ –≤ –±–ª–æ–∫–µ
                mask = (
                    (self.points[:, 0] >= x_start) &
                    (self.points[:, 0] < x_start + self.block_size) &
                    (self.points[:, 1] >= y_start) &
                    (self.points[:, 1] < y_start + self.block_size)
                )
                
                indices = np.where(mask)[0]
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –±–ª–æ–∫–∏ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ
                if len(indices) >= 100:  # –ú–∏–Ω–∏–º—É–º 100 —Ç–æ—á–µ–∫
                    blocks.append({
                        'indices': indices,
                        'x_start': x_start,
                        'y_start': y_start
                    })
                
                y_start += self.stride
            x_start += self.stride
            pbar.update(1)
        
        pbar.close()
        
        self.blocks = blocks
        print(f"   ‚Ä¢ –°–æ–∑–¥–∞–Ω–æ –±–ª–æ–∫–æ–≤: {len(blocks)}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–ª–æ–∫–æ–≤
        block_sizes = [len(b['indices']) for b in blocks]
        print(f"   ‚Ä¢ –¢–æ—á–µ–∫ –≤ –±–ª–æ–∫–µ: min={min(block_sizes)}, max={max(block_sizes)}, avg={np.mean(block_sizes):.0f}")
        
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–ª–æ–∫–æ–≤ –≤ –∫—ç—à...")
        with open(cache_file, 'wb') as f:
            pickle.dump(self.blocks, f)
    
    def __len__(self):
        return len(self.blocks)
    
    def __getitem__(self, idx):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–ª–æ–∫ —Ç–æ—á–µ–∫ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ –º–µ—Ç–∫–∞–º–∏
        Returns:
            points: (num_points, 3+feature_dim) - xyz + –ø—Ä–∏–∑–Ω–∞–∫–∏
            labels: (num_points,) - –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤
        """
        block = self.blocks[idx]
        indices = block['indices']
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ—á–∫–∏ –±–ª–æ–∫–∞
        block_points = self.points[indices].copy()
        block_labels = self.labels[indices].copy()
        
        # –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –±–ª–æ–∫–∞
        centroid = block_points[:, :2].mean(axis=0)
        block_points[:, 0] -= centroid[0]
        block_points[:, 1] -= centroid[1]
        
        # –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ/–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–æ num_points
        if len(block_points) >= self.num_points:
            # Random sampling
            choice = np.random.choice(len(block_points), self.num_points, replace=False)
        else:
            # Repeat points
            choice = np.random.choice(len(block_points), self.num_points, replace=True)
        
        block_points = block_points[choice]
        block_labels = block_labels[choice]
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if self.use_features:
            feature_list = []
            
            # Intensity
            if 'intensity' in self.features:
                intensity = self.features['intensity'][indices][choice]
                if self.normalize:
                    intensity = intensity / 255.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ [0, 1]
                feature_list.append(intensity.reshape(-1, 1))
            
            # Return number
            if 'return_number' in self.features:
                return_num = self.features['return_number'][indices][choice]
                feature_list.append(return_num.reshape(-1, 1))
            
            # Number of returns
            if 'number_of_returns' in self.features:
                num_returns = self.features['number_of_returns'][indices][choice]
                feature_list.append(num_returns.reshape(-1, 1))
            
            if feature_list:
                features = np.concatenate(feature_list, axis=1)
                block_points = np.concatenate([block_points, features], axis=1)
        
        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        if self.augment:
            block_points = self._augment(block_points)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        if self.normalize:
            block_points[:, :3] = self._normalize_coords(block_points[:, :3])
        
        return torch.FloatTensor(block_points), torch.LongTensor(block_labels)
    
    def _normalize_coords(self, coords):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤ [-1, 1]"""
        centroid = coords.mean(axis=0)
        coords = coords - centroid
        max_dist = np.max(np.sqrt(np.sum(coords**2, axis=1)))
        if max_dist > 0:
            coords = coords / max_dist
        return coords
    
    def _augment(self, points):
        """
        –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è LiDAR –¥–∞–Ω–Ω—ã—Ö
        - Random rotation –≤–æ–∫—Ä—É–≥ Z
        - Random scaling
        - Random jittering
        """
        # Rotation –≤–æ–∫—Ä—É–≥ Z –æ—Å–∏
        if np.random.random() > 0.5:
            theta = np.random.uniform(0, 2 * np.pi)
            cos_theta = np.cos(theta)
            sin_theta = np.sin(theta)
            rotation_matrix = np.array([
                [cos_theta, -sin_theta, 0],
                [sin_theta, cos_theta, 0],
                [0, 0, 1]
            ])
            points[:, :3] = points[:, :3] @ rotation_matrix.T
        
        # Random scaling
        if np.random.random() > 0.5:
            scale = np.random.uniform(0.8, 1.2)
            points[:, :3] *= scale
        
        # Jittering
        if np.random.random() > 0.5:
            noise = np.random.normal(0, 0.02, points[:, :3].shape)
            points[:, :3] += noise
        
        return points
    
    def get_class_distribution(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        all_labels = []
        print("\nüìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –≤ –±–ª–æ–∫–∞—Ö...")
        
        # –í—ã–±–æ—Ä–∫–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        sample_size = min(100, len(self))
        indices = np.random.choice(len(self), sample_size, replace=False)
        
        for i in tqdm(indices, desc="–ü–æ–¥—Å—á–µ—Ç –∫–ª–∞—Å—Å–æ–≤"):
            _, labels = self[i]
            all_labels.append(labels.numpy())
        
        all_labels = np.concatenate(all_labels)
        unique, counts = np.unique(all_labels, return_counts=True)
        
        distribution = {}
        total = len(all_labels)
        
        print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (–≤—ã–±–æ—Ä–∫–∞):")
        for cls, count in zip(unique, counts):
            percent = 100.0 * count / total
            distribution[int(cls)] = int(count)
            if self.dataset_config:
                class_name = self.dataset_config.get_class_name(cls)
                print(f"   –ö–ª–∞—Å—Å {cls} ({class_name}): {count:,} —Ç–æ—á–µ–∫ ({percent:.2f}%)")
            else:
                print(f"   –ö–ª–∞—Å—Å {cls}: {count:,} —Ç–æ—á–µ–∫ ({percent:.2f}%)")
        
        return distribution