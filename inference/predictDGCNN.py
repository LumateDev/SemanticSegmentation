"""
–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π DGCNN –º–æ–¥–µ–ª–∏ –∫ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º LiDAR –¥–∞–Ω–Ω—ã–º
–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
"""

import torch
import numpy as np
import laspy
from pathlib import Path
import argparse
import sys
import os
from tqdm import tqdm
import json
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import warnings
warnings.filterwarnings('ignore')

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.modelDGCNN import DGCNN_LiDAR


# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================

class Config:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    
    # –ü—É—Ç–∏
    CHECKPOINT_PATH = None  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    UNLABELED_DIR = Path('datasets/unlabeled')
    PREDICTED_DIR = Path('datasets/predicted')
    VISUALIZATION_DIR = Path('datasets/predicted/visualizations')
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
    NUM_POINTS = 4096
    BLOCK_SIZE = 50.0
    STRIDE = 25.0  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è
    BATCH_SIZE = 16
    USE_FEATURES = True
    FEATURE_DIM = 3
    
    # Voting –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –±–ª–æ–∫–æ–≤
    USE_VOTING = True  # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ –∑–æ–Ω–∞—Ö –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    VISUALIZE = True
    VIZ_POINT_SIZE = 1
    VIZ_DPI = 150
    VIZ_SAMPLE_POINTS = 50000  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (–¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
    
    # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤
    CLASS_NAMES = {
        0: 'Unclassified',
        1: 'Ground',
        2: 'Vegetation',
        3: 'Building'
    }
    
    CLASS_COLORS = {
        0: [128, 128, 128],  # –°–µ—Ä—ã–π
        1: [139, 69, 19],     # –ö–æ—Ä–∏—á–Ω–µ–≤—ã–π (–∑–µ–º–ª—è)
        2: [34, 139, 34],     # –ó–µ–ª–µ–Ω—ã–π (—Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å)
        3: [255, 0, 0]        # –ö—Ä–∞—Å–Ω—ã–π (–∑–¥–∞–Ω–∏—è)
    }
    
    # –û–±—Ä–∞—Ç–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥: 0,1,2,3 -> 1,2,5,6 (–¥–ª—è LAS —Ñ–∞–π–ª–∞)
    CLASS_REVERSE_MAPPING = {0: 1, 1: 2, 2: 5, 3: 6}


# ==================== –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò ====================

def load_model(checkpoint_path, device='cuda'):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
    
    Args:
        checkpoint_path: –ø—É—Ç—å –∫ .pth —Ñ–∞–π–ª—É
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    
    Returns:
        model: –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≤ eval —Ä–µ–∂–∏–º–µ
        config: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    """
    print(f"\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑: {checkpoint_path}")
    
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    # ========== –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: PyTorch 2.6 compatibility ==========
    # –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ —Å weights_only=False (–±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è –Ω–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π)
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    except TypeError:
        # –î–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π PyTorch (< 2.6) –≥–¥–µ –Ω–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ weights_only
        checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    train_config = checkpoint.get('config', {})
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = DGCNN_LiDAR(
        num_classes=train_config.get('num_classes', 4),
        k=train_config.get('k_neighbors', 20),
        use_features=train_config.get('use_features', True),
        feature_dim=train_config.get('feature_dim', 3),
        dropout=0.0  # Dropout –≤—ã–∫–ª—é—á–µ–Ω –¥–ª—è inference
    )
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    print(f"   üìä –≠–ø–æ—Ö–∞: {checkpoint.get('epoch', 'unknown')}")
    
    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    best_val_acc = checkpoint.get('best_val_acc', None)
    best_val_miou = checkpoint.get('best_val_miou', None)
    
    if best_val_acc is not None:
        print(f"   üéØ Val Accuracy: {best_val_acc:.2f}%")
    if best_val_miou is not None:
        print(f"   üî∑ Val mIoU: {best_val_miou:.2f}%")
    
    print(f"   üîß –ö–ª–∞—Å—Å–æ–≤: {train_config.get('num_classes', 4)}")
    print(f"   üìê K —Å–æ—Å–µ–¥–µ–π: {train_config.get('k_neighbors', 20)}")
    
    return model, train_config

# ==================== –û–ë–†–ê–ë–û–¢–ö–ê LAS –§–ê–ô–õ–ê ====================

class LASPredictor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –≤ LAS —Ñ–∞–π–ª–∞—Ö"""
    
    def __init__(self, model, config, device='cuda'):
        self.model = model
        self.config = config
        self.device = device
        self.class_mapping = {1: 0, 2: 1, 5: 2, 6: 3}  # –î–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–∞–∑–º–µ—Ç–∫–∞
    
    def load_las(self, las_file):
        """–ó–∞–≥—Ä—É–∑–∫–∞ LAS —Ñ–∞–π–ª–∞"""
        print(f"\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {las_file}")
        las = laspy.read(las_file)
        
        # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        xyz = np.vstack([
            np.array(las.x, dtype=np.float32),
            np.array(las.y, dtype=np.float32),
            np.array(las.z, dtype=np.float32)
        ]).T
        
        print(f"   ‚Ä¢ –¢–æ—á–µ–∫: {len(xyz):,}")
        print(f"   ‚Ä¢ X: {xyz[:, 0].min():.2f} ‚Üí {xyz[:, 0].max():.2f}")
        print(f"   ‚Ä¢ Y: {xyz[:, 1].min():.2f} ‚Üí {xyz[:, 1].max():.2f}")
        print(f"   ‚Ä¢ Z: {xyz[:, 2].min():.2f} ‚Üí {xyz[:, 2].max():.2f}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = {}
        
        if hasattr(las, 'intensity'):
            features['intensity'] = np.array(las.intensity, dtype=np.float32)
            print(f"   ‚Ä¢ Intensity: {features['intensity'].min():.0f} ‚Üí {features['intensity'].max():.0f}")
        
        if hasattr(las, 'return_number') or hasattr(las, 'return_num'):
            return_num = las.return_number if hasattr(las, 'return_number') else las.return_num
            features['return_number'] = np.array(return_num, dtype=np.float32)
        
        if hasattr(las, 'number_of_returns') or hasattr(las, 'num_returns'):
            num_returns = las.number_of_returns if hasattr(las, 'number_of_returns') else las.num_returns
            features['number_of_returns'] = np.array(num_returns, dtype=np.float32)
        
        return las, xyz, features
    
    def create_blocks(self, xyz):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–ª–æ–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        print(f"\nüî® –°–æ–∑–¥–∞–Ω–∏–µ –±–ª–æ–∫–æ–≤...")
        
        x_min, y_min = xyz[:, 0].min(), xyz[:, 1].min()
        x_max, y_max = xyz[:, 0].max(), xyz[:, 1].max()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        xyz_normalized = xyz.copy()
        xyz_normalized[:, 0] -= x_min
        xyz_normalized[:, 1] -= y_min
        
        blocks = []
        x_start = 0
        
        while x_start < (x_max - x_min):
            y_start = 0
            while y_start < (y_max - y_min):
                # –ú–∞—Å–∫–∞ —Ç–æ—á–µ–∫ –≤ –±–ª–æ–∫–µ
                mask = (
                    (xyz_normalized[:, 0] >= x_start) &
                    (xyz_normalized[:, 0] < x_start + self.config.BLOCK_SIZE) &
                    (xyz_normalized[:, 1] >= y_start) &
                    (xyz_normalized[:, 1] < y_start + self.config.BLOCK_SIZE)
                )
                
                indices = np.where(mask)[0]
                
                if len(indices) >= 10:  # –ú–∏–Ω–∏–º—É–º 10 —Ç–æ—á–µ–∫
                    blocks.append({
                        'indices': indices,
                        'x_start': x_start,
                        'y_start': y_start,
                        'center_x': x_start + self.config.BLOCK_SIZE / 2,
                        'center_y': y_start + self.config.BLOCK_SIZE / 2
                    })
                
                y_start += self.config.STRIDE
            x_start += self.config.STRIDE
        
        print(f"   ‚Ä¢ –°–æ–∑–¥–∞–Ω–æ –±–ª–æ–∫–æ–≤: {len(blocks)}")
        
        return blocks, xyz_normalized, (x_min, y_min)
    
    def prepare_block(self, xyz_norm, features, indices):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–ª–æ–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        block_xyz = xyz_norm[indices].copy()
        
        # –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ
        centroid = block_xyz[:, :2].mean(axis=0)
        block_xyz[:, 0] -= centroid[0]
        block_xyz[:, 1] -= centroid[1]
        
        # –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
        if len(block_xyz) >= self.config.NUM_POINTS:
            choice = np.random.choice(len(block_xyz), self.config.NUM_POINTS, replace=False)
        else:
            choice = np.random.choice(len(block_xyz), self.config.NUM_POINTS, replace=True)
        
        block_xyz = block_xyz[choice]
        selected_indices = indices[choice]
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if self.config.USE_FEATURES:
            feature_list = []
            
            if 'intensity' in features:
                intensity = features['intensity'][selected_indices] / 255.0
                feature_list.append(intensity.reshape(-1, 1))
            
            if 'return_number' in features:
                return_num = features['return_number'][selected_indices]
                feature_list.append(return_num.reshape(-1, 1))
            
            if 'number_of_returns' in features:
                num_returns = features['number_of_returns'][selected_indices]
                feature_list.append(num_returns.reshape(-1, 1))
            
            if feature_list:
                feats = np.concatenate(feature_list, axis=1)
                block_xyz = np.concatenate([block_xyz, feats], axis=1)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        centroid_xyz = block_xyz[:, :3].mean(axis=0)
        block_xyz[:, :3] -= centroid_xyz
        max_dist = np.max(np.sqrt(np.sum(block_xyz[:, :3]**2, axis=1)))
        if max_dist > 0:
            block_xyz[:, :3] /= max_dist
        
        return torch.FloatTensor(block_xyz), selected_indices
    
    @torch.no_grad()
    def predict(self, las_file, output_file=None):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –≤—Å–µ–≥–æ LAS —Ñ–∞–π–ª–∞
        
        Args:
            las_file: –ø—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É LAS —Ñ–∞–π–ª—É
            output_file: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ None, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        
        Returns:
            output_file: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
            predictions: –º–∞—Å—Å–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        """
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        las_original, xyz, features = self.load_las(las_file)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –±–ª–æ–∫–æ–≤
        blocks, xyz_normalized, (x_min, y_min) = self.create_blocks(xyz)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—Å–∏–≤–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        if self.config.USE_VOTING:
            # –î–ª—è voting —Ö—Ä–∞–Ω–∏–º —Å—É–º–º—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ—Å–æ–≤
            predictions_sum = np.zeros((len(xyz), 4), dtype=np.float32)
            predictions_count = np.zeros(len(xyz), dtype=np.int32)
        else:
            predictions = np.zeros(len(xyz), dtype=np.int32)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–ª–æ–∫–æ–≤ –±–∞—Ç—á–∞–º–∏
        print(f"\nüîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤...")
        
        num_batches = (len(blocks) + self.config.BATCH_SIZE - 1) // self.config.BATCH_SIZE
        
        for batch_idx in tqdm(range(num_batches), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π"):
            start_idx = batch_idx * self.config.BATCH_SIZE
            end_idx = min(start_idx + self.config.BATCH_SIZE, len(blocks))
            batch_blocks = blocks[start_idx:end_idx]
            
            batch_data = []
            batch_indices = []
            
            for block in batch_blocks:
                block_tensor, selected_indices = self.prepare_block(
                    xyz_normalized, features, block['indices']
                )
                batch_data.append(block_tensor)
                batch_indices.append(selected_indices)
            
            # Stack –≤ –±–∞—Ç—á
            batch_tensor = torch.stack(batch_data).to(self.device)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            outputs = self.model(batch_tensor)  # (B, N, num_classes)
            
            if self.config.USE_VOTING:
                # Softmax –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                probs = torch.softmax(outputs, dim=-1).cpu().numpy()  # (B, N, num_classes)
                
                for i, indices in enumerate(batch_indices):
                    predictions_sum[indices] += probs[i]
                    predictions_count[indices] += 1
            else:
                # –ü—Ä–æ—Å—Ç–æ argmax
                preds = outputs.argmax(dim=-1).cpu().numpy()  # (B, N)
                
                for i, indices in enumerate(batch_indices):
                    predictions[indices] = preds[i]
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        if self.config.USE_VOTING:
            # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            mask = predictions_count > 0
            predictions_sum[mask] /= predictions_count[mask, np.newaxis]
            predictions = predictions_sum.argmax(axis=1)
            
            # –î–ª—è —Ç–æ—á–µ–∫ –±–µ–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (–Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
            predictions[~mask] = 0
        
        print(f"\n‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        unique, counts = np.unique(predictions, return_counts=True)
        print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤:")
        total = len(predictions)
        for cls, count in zip(unique, counts):
            percent = 100.0 * count / total
            class_name = self.config.CLASS_NAMES.get(int(cls), f'Class {cls}')
            print(f"   {class_name}: {count:,} —Ç–æ—á–µ–∫ ({percent:.2f}%)")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if output_file is None:
            output_file = self.config.PREDICTED_DIR / Path(las_file).name
        
        output_file = self.save_predictions(las_original, predictions, output_file)
        
        return output_file, predictions, xyz
    
    def save_predictions(self, las_original, predictions, output_file):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LAS —Ñ–∞–π–ª–∞ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏"""
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        output_file = Path(output_file)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ LAS —Ñ–∞–π–ª–∞
        las_output = laspy.LasData(las_original.header)
        
        # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö —Ç–æ—á–µ–∫
        las_output.x = las_original.x
        las_output.y = las_original.y
        las_output.z = las_original.z
        
        # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        if hasattr(las_original, 'intensity'):
            las_output.intensity = las_original.intensity
        if hasattr(las_original, 'return_number'):
            las_output.return_number = las_original.return_number
        elif hasattr(las_original, 'return_num'):
            las_output.return_num = las_original.return_num
        if hasattr(las_original, 'number_of_returns'):
            las_output.number_of_returns = las_original.number_of_returns
        elif hasattr(las_original, 'num_returns'):
            las_output.num_returns = las_original.num_returns
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        # –û–±—Ä–∞—Ç–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥: 0,1,2,3 -> 1,2,5,6
        predictions_remapped = np.array([
            self.config.CLASS_REVERSE_MAPPING[p] for p in predictions
        ], dtype=np.uint8)
        
        las_output.classification = predictions_remapped
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        las_output.write(str(output_file))
        
        print(f"   ‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
        print(f"   üì¶ –†–∞–∑–º–µ—Ä: {output_file.stat().st_size / 1024 / 1024:.2f} MB")
        
        return output_file


# ==================== –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ====================

def visualize_predictions(xyz, predictions, config, output_path=None, title="Predicted Classes"):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏
    
    Args:
        xyz: –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ç–æ—á–µ–∫ (N, 3)
        predictions: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã (N,)
        config: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        output_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        title: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä–∞—Ñ–∏–∫–∞
    """
    print(f"\nüé® –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")
    
    # –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    if len(xyz) > config.VIZ_SAMPLE_POINTS:
        indices = np.random.choice(len(xyz), config.VIZ_SAMPLE_POINTS, replace=False)
        xyz_viz = xyz[indices]
        pred_viz = predictions[indices]
    else:
        xyz_viz = xyz
        pred_viz = predictions
    
    # –¶–≤–µ—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏
    colors = np.array([config.CLASS_COLORS[p] for p in pred_viz]) / 255.0
    
    # –°–æ–∑–¥–∞–Ω–∏–µ 3D –≥—Ä–∞—Ñ–∏–∫–∞
    fig = plt.figure(figsize=(20, 15))
    
    # 3D –≤–∏–¥ —Å–≤–µ—Ä—Ö—É
    ax1 = fig.add_subplot(221, projection='3d')
    ax1.scatter(xyz_viz[:, 0], xyz_viz[:, 1], xyz_viz[:, 2], 
                c=colors, s=config.VIZ_POINT_SIZE, alpha=0.6)
    ax1.set_xlabel('X')
    ax1.set_ylabel('Y')
    ax1.set_zlabel('Z')
    ax1.set_title('3D View (Top)')
    ax1.view_init(elev=90, azim=-90)
    
    # 3D –≤–∏–¥ —Å–±–æ–∫—É
    ax2 = fig.add_subplot(222, projection='3d')
    ax2.scatter(xyz_viz[:, 0], xyz_viz[:, 1], xyz_viz[:, 2], 
                c=colors, s=config.VIZ_POINT_SIZE, alpha=0.6)
    ax2.set_xlabel('X')
    ax2.set_ylabel('Y')
    ax2.set_zlabel('Z')
    ax2.set_title('3D View (Side)')
    ax2.view_init(elev=10, azim=-45)
    
    # 2D –≤–∏–¥ —Å–≤–µ—Ä—Ö—É (XY)
    ax3 = fig.add_subplot(223)
    ax3.scatter(xyz_viz[:, 0], xyz_viz[:, 1], 
                c=colors, s=config.VIZ_POINT_SIZE, alpha=0.6)
    ax3.set_xlabel('X')
    ax3.set_ylabel('Y')
    ax3.set_title('2D View (Top - XY)')
    ax3.set_aspect('equal')
    
    # –õ–µ–≥–µ–Ω–¥–∞ —Å –∫–ª–∞—Å—Å–∞–º–∏
    ax4 = fig.add_subplot(224)
    ax4.axis('off')
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
    unique, counts = np.unique(pred_viz, return_counts=True)
    total = len(pred_viz)
    
    legend_text = f"{title}\n\n"
    legend_text += f"Total points visualized: {len(xyz_viz):,}\n\n"
    legend_text += "Class Distribution:\n"
    legend_text += "-" * 40 + "\n"
    
    y_pos = 0.9
    for cls, count in zip(unique, counts):
        percent = 100.0 * count / total
        class_name = config.CLASS_NAMES.get(int(cls), f'Class {cls}')
        color = np.array(config.CLASS_COLORS[cls]) / 255.0
        
        # –¶–≤–µ—Ç–Ω–æ–π –∫–≤–∞–¥—Ä–∞—Ç–∏–∫
        ax4.add_patch(plt.Rectangle((0.1, y_pos - 0.02), 0.05, 0.05, 
                                     facecolor=color, edgecolor='black'))
        
        # –¢–µ–∫—Å—Ç
        ax4.text(0.2, y_pos, f"{class_name}: {count:,} ({percent:.1f}%)", 
                fontsize=12, verticalalignment='center')
        
        y_pos -= 0.1
    
    plt.suptitle(title, fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    if output_path is None:
        output_path = config.VISUALIZATION_DIR / f"prediction_{Path(title).stem}.png"
    
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    plt.savefig(output_path, dpi=config.VIZ_DPI, bbox_inches='tight')
    plt.close()
    
    print(f"   ‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
    
    return output_path


# ==================== MAIN ====================

def main():
    parser = argparse.ArgumentParser(description='DGCNN LiDAR Prediction')
    parser.add_argument('--checkpoint', type=str, required=True,
                        help='Path to model checkpoint (.pth)')
    parser.add_argument('--input', type=str, default=None,
                        help='Input LAS file (if None, process all in unlabeled/)')
    parser.add_argument('--output', type=str, default=None,
                        help='Output LAS file path')
    parser.add_argument('--batch_size', type=int, default=16,
                        help='Batch size for inference')
    parser.add_argument('--no_visualize', action='store_true',
                        help='Disable visualization')
    parser.add_argument('--device', type=str, default='cuda',
                        help='Device: cuda or cpu')
    
    args = parser.parse_args()
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config = Config()
    config.BATCH_SIZE = args.batch_size
    config.VISUALIZE = not args.no_visualize
    
    # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"\n{'='*80}")
    print(f"{'üéØ DGCNN LIDAR PREDICTION':^80}")
    print(f"{'='*80}")
    print(f"\nüñ•Ô∏è  Device: {device}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model, train_config = load_model(args.checkpoint, device)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞
    predictor = LASPredictor(model, config, device)
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    if args.input:
        input_files = [Path(args.input)]
    else:
        input_files = list(config.UNLABELED_DIR.glob('*.las'))
        if not input_files:
            print(f"\n‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ LAS —Ñ–∞–π–ª–æ–≤ –≤: {config.UNLABELED_DIR}")
            return
    
    print(f"\nüìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(input_files)}")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
    for las_file in input_files:
        print(f"\n{'='*80}")
        print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {las_file.name}")
        print(f"{'='*80}")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        output_file, predictions, xyz = predictor.predict(
            las_file,
            output_file=args.output
        )
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        if config.VISUALIZE:
            viz_path = config.VISUALIZATION_DIR / f"{las_file.stem}_predicted.png"
            visualize_predictions(
                xyz, predictions, config,
                output_path=viz_path,
                title=f"Predictions: {las_file.name}"
            )
    
    print(f"\n{'='*80}")
    print(f"‚úÖ –í–°–ï –§–ê–ô–õ–´ –û–ë–†–ê–ë–û–¢–ê–ù–´")
    print(f"{'='*80}")
    print(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"   ‚Ä¢ Predicted LAS: {config.PREDICTED_DIR}")
    if config.VISUALIZE:
        print(f"   ‚Ä¢ Visualizations: {config.VISUALIZATION_DIR}")


if __name__ == '__main__':
    main()