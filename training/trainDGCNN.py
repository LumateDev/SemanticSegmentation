"""
–°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è DGCNN –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ LiDAR –¥–∞–Ω–Ω—ã—Ö

–°—Ç—Ä—É–∫—Ç—É—Ä–∞:
    logs/DGCNN/{timestamp}/
    checkpoints/DGCNN/{timestamp}/
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm
import os
import sys
import json
import argparse
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt
import traceback

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.modelDGCNN import DGCNN_LiDAR
from utils.dataset import LASDataset
from utils.losses import FocalLoss, compute_class_weights
from utils.metrics import SegmentationMetrics


# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================

def get_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    config = {
        # ========== –î–ê–ù–ù–´–ï ==========
        'las_file': 'datasets/raw/NEONDSSampleLiDARPointCloud.las',
        'dataset_config': 'configs/datasets/neon_sample.yaml',
        'num_points': 4096,
        'block_size': 50.0,
        'stride': 25.0,
        'use_features': True,
        'feature_dim': 3,  # intensity, return_number, number_of_returns
        'normalize': True,
        'train_ratio': 0.8,
        
        # ========== –ú–û–î–ï–õ–¨ ==========
        'model_name': 'DGCNN',
        'num_classes': 4,
        'k_neighbors': 20,
        'dropout': 0.5,
        
        # ========== –û–ë–£–ß–ï–ù–ò–ï ==========
        'batch_size': 8,
        'epochs': 3,
        'learning_rate': 0.001,
        'weight_decay': 1e-4,
        'grad_clip': 1.0,
        
        # ========== LOSS ==========
        'loss_type': 'focal',  # 'focal', 'ce', 'dice'
        'use_class_weights': True,
        'focal_gamma': 2.0,
        'weight_mode': 'effective',  # 'inverse', 'effective', 'sqrt'
        
        # ========== OPTIMIZER & SCHEDULER ==========
        'optimizer': 'adamw',  # 'adam', 'adamw', 'sgd'
        'scheduler': 'cosine',  # 'cosine', 'step', 'plateau', None
        'min_lr': 1e-6,
        'patience': 10,  # –¥–ª—è ReduceLROnPlateau
        
        # ========== EARLY STOPPING ==========
        'early_stopping': True,
        'early_stopping_patience': 15,
        'early_stopping_delta': 0.001,
        
        # ========== AUGMENTATION ==========
        'augment_train': True,
        'augment_val': False,
        
        # ========== –î–†–£–ì–û–ï ==========
        'num_workers': 4,
        'pin_memory': True if torch.cuda.is_available() else False,  # –¢–æ–ª—å–∫–æ –¥–ª—è GPU
        'save_freq': 5,  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ N —ç–ø–æ—Ö
        'seed': 42,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    }
    
    return config


# ==================== EARLY STOPPING ====================

class EarlyStopping:
    """Early stopping –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, patience=15, delta=0.001, verbose=True):
        self.patience = patience
        self.delta = delta
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_epoch = 0
    
    def __call__(self, score, epoch):
        if self.best_score is None:
            self.best_score = score
            self.best_epoch = epoch
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.verbose:
                print(f"   ‚ö†Ô∏è  EarlyStopping counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.best_epoch = epoch
            self.counter = 0


# ==================== TRAINER ====================

class DGCNNTrainer:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è DGCNN"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device(config['device'])
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_name = f"DGCNN_{self.timestamp}"
        
        self.log_dir = Path('logs') / 'DGCNN' / self.run_name
        self.checkpoint_dir = Path('checkpoints') / 'DGCNN' / self.run_name
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(log_dir=str(self.log_dir))
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.history = {
            'train_loss': [],
            'train_acc': [],
            'train_miou': [],
            'val_loss': [],
            'val_acc': [],
            'val_miou': [],
            'lr': []
        }
        
        # –õ—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        self.best_val_acc = 0.0
        self.best_val_miou = 0.0
        self.best_epoch = 0
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.train_metrics = SegmentationMetrics(num_classes=config['num_classes'])
        self.val_metrics = SegmentationMetrics(num_classes=config['num_classes'])
        
        # Early stopping
        if config['early_stopping']:
            self.early_stopping = EarlyStopping(
                patience=config['early_stopping_patience'],
                delta=config['early_stopping_delta']
            )
        else:
            self.early_stopping = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        self._print_header()
        self._set_seed()
        self._setup_data()
        self._setup_model()
        self._setup_optimizer()
        self._save_config()
    
    def _print_header(self):
        """–ü–µ—á–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        print("\n" + "="*80)
        print(f"{'üöÄ DGCNN LIDAR SEMANTIC SEGMENTATION':^80}")
        print("="*80)
        print(f"\nüìÖ Timestamp: {self.timestamp}")
        print(f"üìÅ Logs: {self.log_dir}")
        print(f"üíæ Checkpoints: {self.checkpoint_dir}")
        print(f"üñ•Ô∏è  Device: {self.device}")
        
        if self.device.type == 'cuda':
            print(f"   GPU: {torch.cuda.get_device_name(0)}")
            print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    def _set_seed(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏"""
        seed = self.config['seed']
        torch.manual_seed(seed)
        np.random.seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
        print(f"\nüé≤ Random seed: {seed}")
    
    def _setup_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("\n" + "="*80)
        print("üì¶ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
        print("="*80)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞
        if not os.path.exists(self.config['las_file']):
            raise FileNotFoundError(f"LAS file not found: {self.config['las_file']}")
        
        # –ó–ê–ì–†–£–ó–ö–ê –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò –î–ê–¢–ê–°–ï–¢–ê
        from utils.dataset_config import DatasetConfig, auto_detect_config
        
        dataset_config_path = self.config.get('dataset_config', None)
        
        if dataset_config_path and os.path.exists(dataset_config_path):
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            dataset_config = DatasetConfig(dataset_config_path)
            print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {dataset_config_path}")
        else:
            # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            dataset_config = auto_detect_config(self.config['las_file'])
            
            if dataset_config is None:
                # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º NEON –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                print("‚ö†Ô∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NEON –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
                neon_config = Path('configs/datasets/neon_sample.yaml')
                if neon_config.exists():
                    dataset_config = DatasetConfig(neon_config)
                else:
                    raise FileNotFoundError(
                        "Dataset config not found! Create configs/datasets/neon_sample.yaml"
                    )
        
        # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        dataset_config.print_info()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º num_classes –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
        self.config['num_classes'] = dataset_config.num_classes
        self.train_metrics = SegmentationMetrics(num_classes=dataset_config.num_classes)
        self.val_metrics = SegmentationMetrics(num_classes=dataset_config.num_classes)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        full_dataset = LASDataset(
            las_file=self.config['las_file'],
            num_points=self.config['num_points'],
            block_size=self.config['block_size'],
            stride=self.config['stride'],
            use_features=self.config['use_features'],
            normalize=self.config['normalize'],
            augment=False,
            dataset_config=dataset_config  # üÜï –ü–ï–†–ï–î–ê–ï–ú –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Æ
        )
        
        # Train/Val split
        train_size = int(self.config['train_ratio'] * len(full_dataset))
        val_size = len(full_dataset) - train_size
        
        generator = torch.Generator().manual_seed(self.config['seed'])
        train_dataset, val_dataset = random_split(
            full_dataset, [train_size, val_size], generator=generator
        )
        
        # –í–∫–ª—é—á–∞–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è train
        if self.config['augment_train']:
            full_dataset.augment = True
        
        print(f"\nüìä –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   Train: {train_size} –±–ª–æ–∫–æ–≤ ({self.config['train_ratio']*100:.0f}%)")
        print(f"   Val:   {val_size} –±–ª–æ–∫–æ–≤ ({(1-self.config['train_ratio'])*100:.0f}%)")
        
        # DataLoaders
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config['num_workers'],
            pin_memory=self.config['pin_memory'],
            drop_last=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=self.config['pin_memory']
        )
        
        print(f"\n‚úÖ DataLoaders —Å–æ–∑–¥–∞–Ω—ã:")
        print(f"   Train batches: {len(self.train_loader)}")
        print(f"   Val batches:   {len(self.val_loader)}")
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤
        if self.config['use_class_weights']:
            print(f"\n‚öñÔ∏è  –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤...")
            class_dist = full_dataset.get_class_distribution()
            self.class_weights = compute_class_weights(
                class_dist,
                mode=self.config['weight_mode'],
                device=self.device
            )
            print(f"\n   –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ ({self.config['weight_mode']} mode):")
            for i, w in enumerate(self.class_weights):
                class_name = dataset_config.get_class_name(i)
                print(f"   –ö–ª–∞—Å—Å {i} ({class_name}): {w:.4f}")
        else:
            self.class_weights = None
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç–∞—Ö
        self.dataset_config = dataset_config    


    def _setup_model(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ loss"""
        print("\n" + "="*80)
        print("üß† –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò")
        print("="*80)
        
        # –ú–æ–¥–µ–ª—å
        self.model = DGCNN_LiDAR(
            num_classes=self.config['num_classes'],
            k=self.config['k_neighbors'],
            use_features=self.config['use_features'],
            feature_dim=self.config['feature_dim'],
            dropout=self.config['dropout']
        ).to(self.device)
        
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"\nüìä –ú–æ–¥–µ–ª—å: DGCNN")
        print(f"   –ö–ª–∞—Å—Å–æ–≤: {self.config['num_classes']}")
        print(f"   K —Å–æ—Å–µ–¥–µ–π: {self.config['k_neighbors']}")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
        print(f"   –û–±—É—á–∞–µ–º—ã—Ö: {trainable_params:,}")
        print(f"   –†–∞–∑–º–µ—Ä: {total_params * 4 / 1024 / 1024:.2f} MB")
        
        # Loss function
        print(f"\nüìâ Loss function: {self.config['loss_type'].upper()}")
        
        if self.config['loss_type'] == 'focal':
            self.criterion = FocalLoss(
                alpha=self.class_weights,
                gamma=self.config['focal_gamma']
            )
            print(f"   Gamma: {self.config['focal_gamma']}")
        else:
            self.criterion = nn.CrossEntropyLoss(weight=self.class_weights)
        
        print(f"   Class weights: {self.config['use_class_weights']}")
    
    def _setup_optimizer(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ optimizer –∏ scheduler"""
        print("\n" + "="*80)
        print("üéØ OPTIMIZER & SCHEDULER")
        print("="*80)
        
        # Optimizer
        lr = self.config['learning_rate']
        wd = self.config['weight_decay']
        
        if self.config['optimizer'] == 'adamw':
            self.optimizer = optim.AdamW(
                self.model.parameters(),
                lr=lr,
                weight_decay=wd,
                betas=(0.9, 0.999)
            )
        elif self.config['optimizer'] == 'adam':
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=lr,
                weight_decay=wd
            )
        else:  # sgd
            self.optimizer = optim.SGD(
                self.model.parameters(),
                lr=lr,
                momentum=0.9,
                weight_decay=wd
            )
        
        print(f"\n‚úÖ Optimizer: {self.config['optimizer'].upper()}")
        print(f"   Learning Rate: {lr}")
        print(f"   Weight Decay: {wd}")
        
        # Scheduler
        if self.config['scheduler'] == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config['epochs'],
                eta_min=self.config['min_lr']
            )
            print(f"\n‚úÖ Scheduler: Cosine Annealing")
            print(f"   Min LR: {self.config['min_lr']}")
        
        elif self.config['scheduler'] == 'step':
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=20,
                gamma=0.5
            )
            print(f"\n‚úÖ Scheduler: Step LR")
        
        elif self.config['scheduler'] == 'plateau':
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='max',
                factor=0.5,
                patience=self.config['patience'],
                verbose=True
            )
            print(f"\n‚úÖ Scheduler: ReduceLROnPlateau")
        
        else:
            self.scheduler = None
            print(f"\n‚úÖ Scheduler: None")
    
    def _save_config(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        config_path = self.checkpoint_dir / 'config.json'
        with open(config_path, 'w') as f:
            json.dump(self.config, f, indent=2)
        print(f"\nüíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config_path}")
    
    def train_epoch(self, epoch):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
        self.model.train()
        self.train_metrics.reset()
        total_loss = 0.0
        
        pbar = tqdm(
            self.train_loader,
            desc=f'Epoch {epoch}/{self.config["epochs"]} [TRAIN]',
            ncols=100,
            ascii=True
        )
        
        for batch_idx, (points, labels) in enumerate(pbar):
            # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ device
            points = points.to(self.device)  # (B, N, C)
            labels = labels.to(self.device)  # (B, N)
            
            # Forward
            self.optimizer.zero_grad()
            outputs = self.model(points)  # (B, N, num_classes)
            
            # Reshape –¥–ª—è loss
            outputs_flat = outputs.reshape(-1, self.config['num_classes'])  # (B*N, num_classes)
            labels_flat = labels.reshape(-1)  # (B*N,)
            
            # Loss
            loss = self.criterion(outputs_flat, labels_flat)
            
            # Backward
            loss.backward()
            
            # Gradient clipping
            if self.config['grad_clip'] > 0:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config['grad_clip']
                )
            
            self.optimizer.step()
            
            # Metrics
            with torch.no_grad():
                preds = outputs_flat.argmax(dim=1)
                self.train_metrics.update(preds, labels_flat)
            
            total_loss += loss.item()
            
            # Progress bar
            if batch_idx % 10 == 0:
                current_metrics = self.train_metrics.get_metrics()
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{current_metrics["overall_acc"]:.1f}%'
                })
        
        # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ —ç–ø–æ—Ö—É
        avg_loss = total_loss / len(self.train_loader)
        metrics = self.train_metrics.get_metrics()
        
        return avg_loss, metrics
    
    @torch.no_grad()
    def validate(self, epoch):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è"""
        self.model.eval()
        self.val_metrics.reset()
        total_loss = 0.0
        
        pbar = tqdm(
            self.val_loader,
            desc=f'Epoch {epoch}/{self.config["epochs"]} [VAL]  ',
            ncols=100,
            ascii=True,
            leave=False
        )
        
        for points, labels in pbar:
            points = points.to(self.device)
            labels = labels.to(self.device)
            
            # Forward
            outputs = self.model(points)
            
            # Reshape
            outputs_flat = outputs.reshape(-1, self.config['num_classes'])
            labels_flat = labels.reshape(-1)
            
            # Loss
            loss = self.criterion(outputs_flat, labels_flat)
            total_loss += loss.item()
            
            # Metrics
            preds = outputs_flat.argmax(dim=1)
            self.val_metrics.update(preds, labels_flat)
        
        avg_loss = total_loss / len(self.val_loader)
        metrics = self.val_metrics.get_metrics()
        
        return avg_loss, metrics
    
    def save_checkpoint(self, epoch, metrics, is_best=False):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'metrics': metrics,
            'history': self.history,
            'config': self.config,
            'best_val_acc': self.best_val_acc,
            'best_val_miou': self.best_val_miou,
            'best_epoch': self.best_epoch
        }
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        last_path = self.checkpoint_dir / 'last_model.pth'
        torch.save(checkpoint, last_path)
        
        # –õ—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        if is_best:
            best_path = self.checkpoint_dir / 'best_model.pth'
            torch.save(checkpoint, best_path)
            print(f"      üíæ Best model saved! (Acc: {self.best_val_acc:.2f}%, mIoU: {self.best_val_miou:.2f}%)")
        
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã
        if epoch % self.config['save_freq'] == 0:
            epoch_path = self.checkpoint_dir / f'model_epoch_{epoch:03d}.pth'
            torch.save(checkpoint, epoch_path)
    
    def plot_history(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
        epochs = range(1, len(self.history['train_loss']) + 1)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'DGCNN Training History - {self.run_name}', fontsize=16, fontweight='bold')
        
        # Loss
        axes[0, 0].plot(epochs, self.history['train_loss'], 'b-', label='Train Loss', linewidth=2)
        axes[0, 0].plot(epochs, self.history['val_loss'], 'r-', label='Val Loss', linewidth=2)
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('Loss History')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Accuracy
        axes[0, 1].plot(epochs, self.history['train_acc'], 'b-', label='Train Acc', linewidth=2)
        axes[0, 1].plot(epochs, self.history['val_acc'], 'r-', label='Val Acc', linewidth=2)
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy (%)')
        axes[0, 1].set_title('Accuracy History')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # mIoU
        axes[1, 0].plot(epochs, self.history['train_miou'], 'b-', label='Train mIoU', linewidth=2)
        axes[1, 0].plot(epochs, self.history['val_miou'], 'r-', label='Val mIoU', linewidth=2)
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('mIoU (%)')
        axes[1, 0].set_title('mIoU History')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # Learning Rate
        axes[1, 1].plot(epochs, self.history['lr'], 'g-', linewidth=2)
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Learning Rate')
        axes[1, 1].set_title('Learning Rate Schedule')
        axes[1, 1].set_yscale('log')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        plot_path = self.checkpoint_dir / 'training_history.png'
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"\nüìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {plot_path}")
    
    def train(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
        print("\n" + "="*80)
        print("üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
        print("="*80)
        
        for epoch in range(1, self.config['epochs'] + 1):
            print(f"\n{'='*80}")
            print(f"üìÖ –≠–ø–æ—Ö–∞ {epoch}/{self.config['epochs']}")
            print(f"{'='*80}")
            
            # Train
            train_loss, train_metrics = self.train_epoch(epoch)
            
            # Validation
            val_loss, val_metrics = self.validate(epoch)
            
            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏ {epoch}:")
            print(f"   {'‚îÄ'*76}")
            print(f"   üìà TRAIN | Loss: {train_loss:.4f} | Acc: {train_metrics['overall_acc']:6.2f}% | mIoU: {train_metrics['mean_iou']:6.2f}%")
            print(f"   üìâ VAL   | Loss: {val_loss:.4f} | Acc: {val_metrics['overall_acc']:6.2f}% | mIoU: {val_metrics['mean_iou']:6.2f}%")
            
            # Per-class –º–µ—Ç—Ä–∏–∫–∏

            print(f"\n   üéØ Per-Class Metrics (Validation):")
            for i in range(self.config['num_classes']):
                acc = val_metrics['class_acc'][i]
                iou = val_metrics['iou_per_class'][i]
                # üÜï –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
                class_name = self.dataset_config.get_class_name(i)
                print(f"      {class_name}: Acc={acc:6.2f}% | IoU={iou:6.2f}%")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_metrics['overall_acc'])
            self.history['train_miou'].append(train_metrics['mean_iou'])
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_metrics['overall_acc'])
            self.history['val_miou'].append(val_metrics['mean_iou'])
            self.history['lr'].append(self.optimizer.param_groups[0]['lr'])
            
            # TensorBoard
            self.writer.add_scalars('Loss', {
                'train': train_loss,
                'val': val_loss
            }, epoch)
            self.writer.add_scalars('Accuracy', {
                'train': train_metrics['overall_acc'],
                'val': val_metrics['overall_acc']
            }, epoch)
            self.writer.add_scalars('mIoU', {
                'train': train_metrics['mean_iou'],
                'val': val_metrics['mean_iou']
            }, epoch)
            self.writer.add_scalar('Learning_Rate', self.optimizer.param_groups[0]['lr'], epoch)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            is_best = val_metrics['overall_acc'] > self.best_val_acc
            if is_best:
                self.best_val_acc = val_metrics['overall_acc']
                self.best_val_miou = val_metrics['mean_iou']
                self.best_epoch = epoch
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
            self.save_checkpoint(epoch, val_metrics, is_best)
            
            # Learning rate scheduler
            if self.scheduler is not None:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_metrics['overall_acc'])
                else:
                    self.scheduler.step()
            
            # Early stopping
            if self.early_stopping is not None:
                self.early_stopping(val_metrics['overall_acc'], epoch)
                if self.early_stopping.early_stop:
                    print(f"\n‚ö†Ô∏è  Early stopping triggered at epoch {epoch}")
                    print(f"   No improvement for {self.early_stopping.patience} epochs")
                    break
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        self._finish_training()
    
    def _finish_training(self):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è"""
        print("\n" + "="*80)
        print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        print("="*80)
        
        print(f"\nüèÜ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(f"   –≠–ø–æ—Ö–∞: {self.best_epoch}")
        print(f"   Accuracy: {self.best_val_acc:.2f}%")
        print(f"   mIoU: {self.best_val_miou:.2f}%")
        
        print(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   Checkpoints: {self.checkpoint_dir}")
        print(f"   Logs: {self.log_dir}")
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
        self.plot_history()
        
        # –ó–∞–∫—Ä—ã—Ç–∏–µ TensorBoard
        self.writer.close()
        
        print(f"\nüí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ TensorBoard –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞:")
        print(f"   tensorboard --logdir=logs/DGCNN")
        print("\n" + "="*80)


# ==================== MAIN ====================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    parser = argparse.ArgumentParser(description='Train DGCNN for LiDAR Segmentation')
    parser.add_argument('--las_file', type=str, default=None, help='Path to LAS file')
    parser.add_argument('--dataset_config', type=str, default=None, help='Path to dataset config YAML')
    parser.add_argument('--batch_size', type=int, default=None, help='Batch size')
    parser.add_argument('--epochs', type=int, default=None, help='Number of epochs')
    parser.add_argument('--lr', type=float, default=None, help='Learning rate')
    args = parser.parse_args()
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config = get_config()
    
     # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    if args.las_file is not None:
        config['las_file'] = args.las_file
    if args.dataset_config is not None:
        config['dataset_config'] = args.dataset_config
    if args.batch_size is not None:
        config['batch_size'] = args.batch_size
    if args.epochs is not None:
        config['epochs'] = args.epochs
    if args.lr is not None:
        config['learning_rate'] = args.lr
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞
    if not os.path.exists(config['las_file']):
        print(f"\n‚ùå –û—à–∏–±–∫–∞: LAS —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {config['las_file']}")
        print("\nüí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–∞–ø–∫–µ:")
        print("   datasets/raw/NEONDSSampleLiDARPointCloud.las")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    try:
        trainer = DGCNNTrainer(config)
        trainer.train()
    
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        print("   –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
    
    except Exception as e:
        print(f"\n\n‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:")
        print(f"   {e}")
        traceback.print_exc()


if __name__ == '__main__':
    main()